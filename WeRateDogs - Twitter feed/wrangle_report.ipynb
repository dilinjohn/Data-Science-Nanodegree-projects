{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project I will describe our wrangling effort made in the section of wrangling weRateDog project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data wrangling has 3 main steps:\n",
    "- Gather\n",
    "- Assess\n",
    "- Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering Data for this Project composed from three pieces of data as described below:\n",
    "\n",
    "- The WeRateDogs Twitter archive. We manually downloaded this file manually by clicking the following link: twitter_archive_enhanced.csv\n",
    "\n",
    "- The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) hosted on Udacity's servers and we downloaded it programmatically using python Requests library on the following (URL of the file: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "- Using the tweet IDs in the WeRateDogs Twitter archive, we could query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data stored in a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering - next steps\n",
    "- Looking at the data\n",
    "    - Getting data from an existing file (twitter-archive-enhanced.csv).Reading from csv file using pandas\n",
    "    - Downloading a file from a url, hosted on udacity servers(image-predictions.tsv) Downloading file using requests.\n",
    "    - Querying using tweepy API (tweet_json.txt) Get JSON object of all the tweet_ids in twitter-archive-enhanced.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering each of the above data, the next step is to assess  them visually and programmatically for quality and tidiness issues. There were some quality issues and tidiness issues that were detected and they have been documented as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness, Validity, Accuracy, Consistency  or content issues archive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Incorrect dog names. dog names in lower cases are mostly incorrect\n",
    "- Rating numerators with decimals are incorrect\n",
    "- Tweet with more than one XX.XX /XX sometimes have the first occurence erroneously used for the rating numerators and denominators\n",
    "- Tweets with no images\n",
    "- Dataset contains retweets\n",
    "- We only want original ratings (no retweets) that have images\n",
    "- Contents of 'text' appear to be cut of but this is fixed by changing the column width\n",
    "- Incorrect datatypes fixed \n",
    "- Tweet ID# 810984652412424192 doesn't contain a rating. Rating captured in numerator /denominator is improper\n",
    "- Some tweet_ids duplicated\n",
    "- Some columns have None. Change to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness issue - This mean untidy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No need to all the informations in images dataset, (tweet_id and jpg_url what matters)\n",
    "- Various stages of dogs in columns instead of rows archives dataset\n",
    "- We may want to add a gender column from the text columns in archives dataset\n",
    "- All tables should be part of one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step in our data wrangling process is cleaning the data. It is where we fixed the quality and tidiness issues that we identified in the assess step. There were two types of cleaning approaches, the manual and programmatic even the manual not recommended but the issues were one-off occurrences. Our process was Define, Code and Test and we were always making a copy of tha dataset even we made the copy in file to test the change before applying to the main dataset. All the quality and tidiness were not spotted during the assessment phase and so we have been iterating and revisiting assessing to add these assessments to our notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from 3 different data source, including one which included API calls to twitter to get more details based on a tweet id, data wrangling was performed after gathering, assessing and cleaning up the data which forms quite significant part of this project. For data visualization , corrleation plot was explored and different types of charts plotted. If we analyze, visualize, or model our data before we wrangle it, our consequences could be making mistakes, missing out on cool insights, and wasting time. So best practice is to wrangle data before visualizing and concluding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
